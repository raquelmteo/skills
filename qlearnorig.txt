import numpy as np
import matplotlib.pyplot as plt

# Define the size of the grid
GRID_SIZE_X = 20
GRID_SIZE_Y = 20
EPISODES=10000

class MapEnvironment:
    def __init__(self):
        self.grid = np.zeros((GRID_SIZE_X, GRID_SIZE_Y))  # Initialize empty grid
        self.death_coordinates = [(5, 4), (5, 5), (5, 6)]  # Define death coordinates
        self.obstacles = [(3, 3), (3, 4),(3,5)]  # Define obstacles
        for coord in self.death_coordinates:
            self.grid[coord] = -2000  # Set death coordinates to -100
        for obstacle in self.obstacles:
            self.grid[obstacle] = -1000  # Set obstacles to -10

    def is_valid_location(self, x, y):
        return 0 <= x < GRID_SIZE_X and 0 <= y < GRID_SIZE_Y and (x, y) not in self.obstacles

    def is_death_location(self, x, y):
        return (x, y) in self.death_coordinates

    def is_goal_location(self, x, y):
        return x == GRID_SIZE_X - 1 and y == GRID_SIZE_Y - 1
    
    def print_grid(self, agent_position):
        for i in range(GRID_SIZE_X):
            for j in range(GRID_SIZE_Y):
                if (i, j) == agent_position:
                    print("A", end=" ")  # Represent agent position
                elif (i, j) in self.death_coordinates:
                    print("X", end=" ")  # Represent death coordinates
                elif (i, j) in self.obstacles:
                    print("O", end=" ")  # Represent obstacles
                elif i == GRID_SIZE_X - 1 and j == GRID_SIZE_Y - 1:
                    print("G", end=" ")  # Represent goal location
                else:
                    print(".", end=" ")  # Represent empty space
            print()
        print()


    def get_reward(self, x, y):
        if self.is_death_location(x, y):
            return -2000
        elif self.is_goal_location(x, y):
            return 5000
        else:
            return -1

class QLearningAgent:
    def __init__(self, environment, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):
        self.environment = environment
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.q_table = np.zeros((GRID_SIZE_X, GRID_SIZE_Y, 4))  # Initialize Q-table with zeros

    def choose_action(self, x, y):
        if np.random.uniform(0, 1) < self.exploration_rate:
            return np.random.choice(4)  # Explore randomly
        else:
            return np.argmax(self.q_table[x, y, :])  # Choose the action with the highest Q-value

    def update_q_table(self, state, action, reward, next_state):
        max_next_action = np.argmax(self.q_table[next_state[0], next_state[1], :])
        current_q_value = self.q_table[state[0], state[1], action]
        self.q_table[state[0], state[1], action] += self.learning_rate * (
                reward + self.discount_factor * self.q_table[next_state[0], next_state[1], max_next_action] - current_q_value)

    def train(self, episodes):
        for episode in range(episodes):
            moves=0
            state = (0, 0)  # Start at the top-left corner
            while not self.environment.is_goal_location(state[0], state[1]):
                moves +=1
                action = self.choose_action(state[0], state[1])
                x, y = state
                if action == 0:  # Up
                    next_state = (x - 1, y)
                elif action == 1:  # Down
                    next_state = (x + 1, y)
                elif action == 2:  # Left
                    next_state = (x, y - 1)
                else:  # Right
                    next_state = (x, y + 1)

                if self.environment.is_valid_location(next_state[0], next_state[1]):
                    reward = self.environment.get_reward(next_state[0], next_state[1])
                    self.update_q_table(state, action, reward, next_state)
                    state = next_state
            print(f"Episode {episode + 1}/{episodes} completed {reward}/{moves}")
   
   

    def test(self):
        state = (0, 0)  # Start at the top-left corner
        states_visited = [state]  # Store visited states for plotting
        while not self.environment.is_goal_location(state[0], state[1]):
            action = np.argmax(self.q_table[state[0], state[1], :])
            x, y = state
            if action == 0:  # Up
                next_state = (x - 1, y)
            elif action == 1:  # Down
                next_state = (x + 1, y)
            elif action == 2:  # Left
                next_state = (x, y - 1)
            else:  # Right
                next_state = (x, y + 1)

            if self.environment.is_valid_location(next_state[0], next_state[1]):
                state = next_state
                states_visited.append(state)

        # Create a grid plot
        grid = self.environment.grid.copy()
        for state in states_visited:
            grid[state] = 1

        plt.imshow(grid, cmap='viridis', interpolation='nearest')
        plt.title('Agent\'s Movement on the Grid')
        plt.xlabel('X-axis')
        plt.ylabel('Y-axis')
        plt.colorbar(label='State Value')
        plt.grid(visible=True)
        plt.show()

if __name__ == "__main__":
    environment = MapEnvironment()
    agent = QLearningAgent(environment)
    agent.train(episodes=EPISODES)

    state = (0, 0)  # Start at the top-left corner
    while not environment.is_goal_location(state[0], state[1]):
        agent_position = state
        environment.print_grid(agent_position)
        action = np.argmax(agent.q_table[state[0], state[1], :])
        x, y = state
        if action == 0:  # Up
            next_state = (x - 1, y)
        elif action == 1:  # Down
            next_state = (x + 1, y)
        elif action == 2:  # Left
            next_state = (x, y - 1)
        else:  # Right
            next_state = (x, y + 1)

        if environment.is_valid_location(next_state[0], next_state[1]):
            state = next_state
        else:
            break  # Stop if the agent hits an obstacle or goes out of bounds

